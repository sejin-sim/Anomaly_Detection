{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAGMM_Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYp3PyzspivYdAwrKydrXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sejin-sim/Anomaly_Detection/blob/main/DAGMM_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hVlFRBsxmuw"
      },
      "source": [
        "코드 출처 : https://github.com/mperezcarrasco/PyTorch-DAGMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vypL2cwQxy5M",
        "outputId": "bbe450e0-b118-44cf-df77-55a2ac220266"
      },
      "source": [
        "pip install barbar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting barbar\n",
            "  Downloading https://files.pythonhosted.org/packages/48/1f/9b69ce144f484cfa00feb09fa752139658961de6303ea592487738d0b53c/barbar-0.2.1-py3-none-any.whl\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8XpxwO-xA9n"
      },
      "source": [
        "# utils.py\n",
        "\n",
        "import torch\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        torch.nn.init.normal_(m.bias.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVCxuUPpxHQC"
      },
      "source": [
        "# forward_step.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ComputeLoss:\n",
        "    def __init__(self, model, lambda_energy, lambda_cov, device, n_gmm):\n",
        "        self.model = model\n",
        "        self.lambda_energy = lambda_energy\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.device = device\n",
        "        self.n_gmm = n_gmm\n",
        "    \n",
        "    def forward(self, x, x_hat, z, gamma):\n",
        "        \"\"\"Computing the loss function for DAGMM.\"\"\"\n",
        "        reconst_loss = torch.mean((x-x_hat).pow(2))\n",
        "\n",
        "        sample_energy, cov_diag = self.compute_energy(z, gamma)\n",
        "\n",
        "        loss = reconst_loss + self.lambda_energy * sample_energy + self.lambda_cov * cov_diag\n",
        "        return Variable(loss, requires_grad=True)\n",
        "    \n",
        "    def compute_energy(self, z, gamma, phi=None, mu=None, cov=None, sample_mean=True):\n",
        "        \"\"\"Computing the sample energy function\"\"\"\n",
        "        if (phi is None) or (mu is None) or (cov is None):\n",
        "            phi, mu, cov = self.compute_params(z, gamma)\n",
        "\n",
        "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
        "\n",
        "        eps = 1e-12\n",
        "        cov_inverse = []\n",
        "        det_cov = []\n",
        "        cov_diag = 0\n",
        "        for k in range(self.n_gmm):\n",
        "            cov_k = cov[k] + (torch.eye(cov[k].size(-1))*eps).to(self.device)\n",
        "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
        "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
        "            cov_diag += torch.sum(1 / cov_k.diag())\n",
        "        \n",
        "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
        "        det_cov = torch.cat(det_cov).to(self.device)\n",
        "\n",
        "        E_z = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
        "        E_z = torch.exp(E_z)\n",
        "        E_z = -torch.log(torch.sum(phi.unsqueeze(0)*E_z / (torch.sqrt(det_cov)).unsqueeze(0), dim=1) + eps)\n",
        "        if sample_mean==True:\n",
        "            E_z = torch.mean(E_z)            \n",
        "        return E_z, cov_diag\n",
        "\n",
        "    def compute_params(self, z, gamma):\n",
        "        \"\"\"Computing the parameters phi, mu and gamma for sample energy function \"\"\" \n",
        "        # K: number of Gaussian mixture components\n",
        "        # N: Number of samples\n",
        "        # D: Latent dimension\n",
        "        # z = NxD\n",
        "        # gamma = NxK\n",
        "\n",
        "        #phi = D\n",
        "        phi = torch.sum(gamma, dim=0)/gamma.size(0) \n",
        "\n",
        "        #mu = KxD\n",
        "        mu = torch.sum(z.unsqueeze(1) * gamma.unsqueeze(-1), dim=0)\n",
        "        mu /= torch.sum(gamma, dim=0).unsqueeze(-1)\n",
        "\n",
        "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
        "        z_mu_z_mu_t = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
        "        \n",
        "        #cov = K x D x D\n",
        "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_z_mu_t, dim=0)\n",
        "        cov /= torch.sum(gamma, dim=0).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        return phi, mu, cov\n",
        "        \n",
        "\n",
        "class Cholesky(torch.autograd.Function):\n",
        "    def forward(ctx, a):\n",
        "        l = torch.cholesky(a, False)\n",
        "        ctx.save_for_backward(l)\n",
        "        return l\n",
        "    def backward(ctx, grad_output):\n",
        "        l, = ctx.saved_variables\n",
        "        linv = l.inverse()\n",
        "        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
        "            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
        "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
        "        return s"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0In98UQxPHU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DAGMM(nn.Module):\n",
        "    def __init__(self, n_gmm=2, z_dim=1):\n",
        "        \"\"\"Network for DAGMM (KDDCup99)\"\"\"\n",
        "        super(DAGMM, self).__init__()\n",
        "        #Encoder network\n",
        "        self.fc1 = nn.Linear(118, 60)\n",
        "        self.fc2 = nn.Linear(60, 30)\n",
        "        self.fc3 = nn.Linear(30, 10)\n",
        "        self.fc4 = nn.Linear(10, z_dim)\n",
        "\n",
        "        #Decoder network\n",
        "        self.fc5 = nn.Linear(z_dim, 10)\n",
        "        self.fc6 = nn.Linear(10, 30)\n",
        "        self.fc7 = nn.Linear(30, 60)\n",
        "        self.fc8 = nn.Linear(60, 118)\n",
        "\n",
        "        #Estimation network\n",
        "        self.fc9 = nn.Linear(z_dim+2, 10)\n",
        "        self.fc10 = nn.Linear(10, n_gmm)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.tanh(self.fc1(x))\n",
        "        h = torch.tanh(self.fc2(h))\n",
        "        h = torch.tanh(self.fc3(h))\n",
        "        return self.fc4(h)\n",
        "\n",
        "    def decode(self, x):\n",
        "        h = torch.tanh(self.fc5(x))\n",
        "        h = torch.tanh(self.fc6(h))\n",
        "        h = torch.tanh(self.fc7(h))\n",
        "        return self.fc8(h)\n",
        "    \n",
        "    def estimate(self, z):\n",
        "        h = F.dropout(torch.tanh(self.fc9(z)), 0.5)\n",
        "        return F.softmax(self.fc10(h), dim=1)\n",
        "    \n",
        "    def compute_reconstruction(self, x, x_hat):\n",
        "        relative_euclidean_distance = (x-x_hat).norm(2, dim=1) / x.norm(2, dim=1)\n",
        "        cosine_similarity = F.cosine_similarity(x, x_hat, dim=1)\n",
        "        return relative_euclidean_distance, cosine_similarity\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z_c = self.encode(x)\n",
        "        x_hat = self.decode(z_c)\n",
        "        rec_1, rec_2 = self.compute_reconstruction(x, x_hat)\n",
        "        z = torch.cat([z_c, rec_1.unsqueeze(-1), rec_2.unsqueeze(-1)], dim=1)\n",
        "        gamma = self.estimate(z)\n",
        "        return z_c, x_hat, z, gamma"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pZ7OYbPxTep"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle as pl\n",
        "\n",
        "\n",
        "\n",
        "class KDDCupData:\n",
        "    def __init__(self, data_dir, mode):\n",
        "        \"\"\"Loading the data for train and test.\"\"\"\n",
        "        data = np.load(data_dir, allow_pickle=True)\n",
        "\n",
        "        labels = data[\"kdd\"][:,-1]\n",
        "        features = data[\"kdd\"][:,:-1]\n",
        "        #In this case, \"atack\" has been treated as normal data as is mentioned in the paper\n",
        "        normal_data = features[labels==0] \n",
        "        normal_labels = labels[labels==0]\n",
        "\n",
        "        n_train = int(normal_data.shape[0]*0.5)\n",
        "        ixs = np.arange(normal_data.shape[0])\n",
        "        np.random.shuffle(ixs)\n",
        "        normal_data_test = normal_data[ixs[n_train:]]\n",
        "        normal_labels_test = normal_labels[ixs[n_train:]]\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.x = normal_data[ixs[:n_train]]\n",
        "            self.y = normal_labels[ixs[:n_train]]\n",
        "        elif mode == 'test':\n",
        "            anomalous_data = features[labels==1]\n",
        "            anomalous_labels = labels[labels==1]\n",
        "            self.x = np.concatenate((anomalous_data, normal_data_test), axis=0)\n",
        "            self.y = np.concatenate((anomalous_labels, normal_labels_test), axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of images in the object dataset.\"\"\"\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return a sample from the dataset.\"\"\"\n",
        "        return np.float32(self.x[index]), np.float32(self.y[index])\n",
        "\n",
        "\n",
        "\n",
        "def get_KDDCup99(args, data_dir='./data/kdd_cup.npz'):\n",
        "    \"\"\"Returning train and test dataloaders.\"\"\"\n",
        "    train = KDDCupData(data_dir, 'train')\n",
        "    dataloader_train = DataLoader(train, batch_size=args.batch_size, \n",
        "                              shuffle=True, num_workers=0)\n",
        "    \n",
        "    test = KDDCupData(data_dir, 'test')\n",
        "    dataloader_test = DataLoader(test, batch_size=args.batch_size, \n",
        "                              shuffle=False, num_workers=0)\n",
        "    return dataloader_train, dataloader_test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gen1WdPZxWpi"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
        "\n",
        "\n",
        "def eval(model, dataloaders, device, n_gmm):\n",
        "    \"\"\"Testing the DAGMM model\"\"\"\n",
        "    dataloader_train, dataloader_test = dataloaders\n",
        "    model.eval()\n",
        "    print('Testing...')\n",
        "    compute = ComputeLoss(model, None, None, device, n_gmm)\n",
        "    with torch.no_grad():\n",
        "        N_samples = 0\n",
        "        gamma_sum = 0\n",
        "        mu_sum = 0\n",
        "        cov_sum = 0\n",
        "        # Obtaining the parameters gamma, mu and cov using the trainin (clean) data.\n",
        "        for x, _ in dataloader_train:\n",
        "            x = x.float().to(device)\n",
        "\n",
        "            _, _, z, gamma = model(x)\n",
        "            phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
        "\n",
        "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
        "            gamma_sum += batch_gamma_sum\n",
        "            mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
        "            cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
        "            \n",
        "            N_samples += x.size(0)\n",
        "            \n",
        "        train_phi = gamma_sum / N_samples\n",
        "        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
        "        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Obtaining Labels and energy scores for train data\n",
        "        energy_train = []\n",
        "        labels_train = []\n",
        "        for x, y in dataloader_train:\n",
        "            x = x.float().to(device)\n",
        "\n",
        "            _, _, z, gamma = model(x)\n",
        "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, phi=train_phi,\n",
        "                                                              mu=train_mu, cov=train_cov, \n",
        "                                                              sample_mean=False)\n",
        "            \n",
        "            energy_train.append(sample_energy.detach().cpu())\n",
        "            labels_train.append(y)\n",
        "        energy_train = torch.cat(energy_train).numpy()\n",
        "        labels_train = torch.cat(labels_train).numpy()\n",
        "\n",
        "        # Obtaining Labels and energy scores for test data\n",
        "        energy_test = []\n",
        "        labels_test = []\n",
        "        for x, y in dataloader_test:\n",
        "            x = x.float().to(device)\n",
        "\n",
        "            _, _, z, gamma = model(x)\n",
        "            sample_energy, cov_diag  = compute.compute_energy(z, gamma, train_phi,\n",
        "                                                              train_mu, train_cov,\n",
        "                                                              sample_mean=False)\n",
        "            \n",
        "            energy_test.append(sample_energy.detach().cpu())\n",
        "            labels_test.append(y)\n",
        "        energy_test = torch.cat(energy_test).numpy()\n",
        "        labels_test = torch.cat(labels_test).numpy()\n",
        "    \n",
        "        scores_total = np.concatenate((energy_train, energy_test), axis=0)\n",
        "        labels_total = np.concatenate((labels_train, labels_test), axis=0)\n",
        "\n",
        "    threshold = np.percentile(scores_total, 100 - 20)\n",
        "    pred = (energy_test > threshold).astype(int)\n",
        "    gt = labels_test.astype(int)\n",
        "    precision, recall, f_score, _ = prf(gt, pred, average='binary')\n",
        "    print(\"Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(precision, recall, f_score))\n",
        "    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels_total, scores_total)*100))\n",
        "    return labels_total, scores_total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wOhedlfxZar"
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from barbar import Bar\n",
        "\n",
        "class TrainerDAGMM:\n",
        "    \"\"\"Trainer class for DAGMM.\"\"\"\n",
        "    def __init__(self, args, data, device):\n",
        "        self.args = args\n",
        "        self.train_loader, self.test_loader = data\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Training the DAGMM model\"\"\"\n",
        "        self.model = DAGMM(self.args.n_gmm, self.args.latent_dim).to(self.device)\n",
        "        self.model.apply(weights_init_normal)\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
        "\n",
        "        self.compute = ComputeLoss(self.model, self.args.lambda_energy, self.args.lambda_cov, \n",
        "                                   self.device, self.args.n_gmm)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            total_loss = 0\n",
        "            for x, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                _, x_hat, z, gamma = self.model(x)\n",
        "\n",
        "                loss = self.compute.forward(x, x_hat, z, gamma)\n",
        "                loss.backward(retain_graph=True)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "            print('Training DAGMM... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-N0QwU2x-q0",
        "outputId": "7ccc002c-5e34-4f08-cdef-fbd788072b49"
      },
      "source": [
        "! git clone https://github.com/mperezcarrasco/PyTorch-DAGMM.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch-DAGMM'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 102 (delta 47), reused 61 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 1.83 MiB | 1.27 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igB1Bu5EyITE"
      },
      "source": [
        "cp -r /content/PyTorch-DAGMM/data /content"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5xbH3Vxwq7H",
        "outputId": "1ba042b5-280b-41aa-c98a-fe56f964f016"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "class Args:\n",
        "    num_epochs=200\n",
        "    patience=50\n",
        "    lr=1e-4\n",
        "    lr_milestones=[50]\n",
        "    batch_size=1024\n",
        "    latent_dim=1\n",
        "    n_gmm=4\n",
        "    lambda_energy=0.1\n",
        "    lambda_cov=0.005\n",
        "    \n",
        "    \n",
        "args = Args()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = get_KDDCup99(args)\n",
        "\n",
        "dagmm = TrainerDAGMM(args, data, device)\n",
        "dagmm.train()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "198371/198371: [===============================>] - ETA 0.2s\n",
            "Training DAGMM... Epoch: 0, Loss: 160208132.289\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 1, Loss: 160113942.227\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 2, Loss: 160075386.351\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 3, Loss: 159876415.464\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 4, Loss: 160131504.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 5, Loss: 160643000.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 6, Loss: 161017529.897\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 7, Loss: 159882099.175\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 8, Loss: 160184745.237\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 9, Loss: 161019850.928\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 10, Loss: 160375119.299\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 11, Loss: 160863240.330\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 12, Loss: 160945405.567\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 13, Loss: 160413839.052\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 14, Loss: 160978710.722\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 15, Loss: 160889856.330\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 16, Loss: 160678580.825\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 17, Loss: 160316489.278\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 18, Loss: 160273713.320\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 19, Loss: 160549275.629\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 20, Loss: 160535054.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 21, Loss: 160011762.227\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 22, Loss: 160046832.866\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 23, Loss: 160309234.680\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 24, Loss: 160272887.423\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 25, Loss: 160499519.505\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 26, Loss: 160091765.732\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 27, Loss: 160265839.546\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 28, Loss: 160967682.268\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 29, Loss: 160975476.206\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 30, Loss: 161186128.990\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 31, Loss: 159738480.990\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 32, Loss: 159975332.825\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 33, Loss: 160961387.340\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 34, Loss: 160928150.969\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 35, Loss: 160800842.804\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 36, Loss: 160754970.351\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 37, Loss: 160958457.938\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 38, Loss: 159565774.845\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 39, Loss: 160877940.082\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 40, Loss: 160001866.474\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 41, Loss: 160577664.866\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 42, Loss: 160277688.536\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 43, Loss: 160525778.227\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 44, Loss: 161504868.041\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 45, Loss: 160386281.072\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 46, Loss: 159584344.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 47, Loss: 160730225.814\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 48, Loss: 160275600.784\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 49, Loss: 159698289.773\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 50, Loss: 159871546.309\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 51, Loss: 160469747.216\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 52, Loss: 159921806.515\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 53, Loss: 160191223.464\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 54, Loss: 159970957.856\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 55, Loss: 160101566.144\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 56, Loss: 160216823.340\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 57, Loss: 160294231.959\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 58, Loss: 160227839.505\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 59, Loss: 160095323.918\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 60, Loss: 160488084.247\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 61, Loss: 160286108.000\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 62, Loss: 160923567.216\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 63, Loss: 160427624.784\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 64, Loss: 159697960.907\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 65, Loss: 160592467.052\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 66, Loss: 160754822.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 67, Loss: 160470581.526\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 68, Loss: 161170272.330\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 69, Loss: 159902632.742\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 70, Loss: 161033140.577\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 71, Loss: 161026360.784\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 72, Loss: 160817996.082\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 73, Loss: 162221367.505\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 74, Loss: 159702097.567\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 75, Loss: 160218901.443\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 76, Loss: 160196407.175\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 77, Loss: 159810218.021\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 78, Loss: 160166935.340\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 79, Loss: 160138830.722\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 80, Loss: 160140717.567\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 81, Loss: 160955946.103\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 82, Loss: 160157117.155\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 83, Loss: 161139568.412\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 84, Loss: 160065939.959\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 85, Loss: 160240592.660\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 86, Loss: 160752323.670\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 87, Loss: 160468686.392\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 88, Loss: 159156242.557\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 89, Loss: 161249139.711\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 90, Loss: 159806184.371\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 91, Loss: 160182967.052\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 92, Loss: 160380361.072\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 93, Loss: 161300110.062\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 94, Loss: 159448957.526\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 95, Loss: 159363377.196\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 96, Loss: 160218737.608\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 97, Loss: 160675354.845\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 98, Loss: 160837677.691\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 99, Loss: 160461454.598\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 100, Loss: 160806748.825\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 101, Loss: 161176606.268\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 102, Loss: 160233495.629\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 103, Loss: 160106173.278\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 104, Loss: 160393680.000\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 105, Loss: 160153062.392\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 106, Loss: 160267198.268\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 107, Loss: 160041948.536\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 108, Loss: 160251297.485\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 109, Loss: 160472674.309\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 110, Loss: 160708134.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 111, Loss: 160472612.866\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 112, Loss: 160388688.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 113, Loss: 160805047.381\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 114, Loss: 160922217.691\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 115, Loss: 159810039.093\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 116, Loss: 160436475.134\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 117, Loss: 160271386.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 118, Loss: 160916953.278\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 119, Loss: 160181808.660\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 120, Loss: 160694366.557\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 121, Loss: 160776580.948\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 122, Loss: 160069684.124\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 123, Loss: 160320321.608\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 124, Loss: 160821465.155\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 125, Loss: 160911340.082\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 126, Loss: 159794518.474\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 127, Loss: 160471324.289\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 128, Loss: 161082850.598\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 129, Loss: 160670204.577\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 130, Loss: 160810462.928\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 131, Loss: 160976590.969\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 132, Loss: 160630250.021\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 133, Loss: 160191932.041\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 134, Loss: 160879725.113\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 135, Loss: 160201368.165\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 136, Loss: 160919939.918\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 137, Loss: 161432955.546\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 138, Loss: 160293653.938\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 139, Loss: 160473698.433\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 140, Loss: 160888234.887\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 141, Loss: 161257847.959\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 142, Loss: 160623259.629\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 143, Loss: 160682881.649\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 144, Loss: 160366822.887\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 145, Loss: 160491310.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 146, Loss: 160468821.196\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 147, Loss: 160582704.660\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 148, Loss: 159845784.536\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 149, Loss: 159969728.948\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 150, Loss: 161044313.608\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 151, Loss: 161035586.680\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 152, Loss: 160249734.144\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 153, Loss: 160492145.237\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 154, Loss: 160447265.237\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 155, Loss: 160008590.557\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 156, Loss: 160938746.474\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 157, Loss: 161404143.258\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 158, Loss: 159997707.258\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 159, Loss: 160105371.299\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 160, Loss: 160795098.515\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 161, Loss: 160070443.052\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 162, Loss: 160590217.691\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 163, Loss: 160854801.773\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 164, Loss: 160642885.278\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 165, Loss: 160117544.825\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 166, Loss: 160694191.835\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 167, Loss: 160570827.959\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 168, Loss: 160546824.041\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 169, Loss: 160822848.660\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 170, Loss: 160347856.866\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 171, Loss: 160994364.495\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 172, Loss: 160793632.907\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 173, Loss: 160208699.134\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 174, Loss: 160198862.722\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 175, Loss: 160725613.031\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 176, Loss: 160358194.268\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 177, Loss: 160609736.742\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 178, Loss: 160461105.237\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 179, Loss: 160058186.021\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 180, Loss: 161484195.340\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 181, Loss: 160224940.866\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 182, Loss: 160719813.113\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 183, Loss: 160971176.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 184, Loss: 160430875.670\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 185, Loss: 159300803.505\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 186, Loss: 160286352.454\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 187, Loss: 160847104.907\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 188, Loss: 160035583.546\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 189, Loss: 160436596.041\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 190, Loss: 159804347.299\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 191, Loss: 160220329.485\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 192, Loss: 160634606.186\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 193, Loss: 160992056.577\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 194, Loss: 160528537.608\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 195, Loss: 161068409.649\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 196, Loss: 160022518.227\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 197, Loss: 159854398.268\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 198, Loss: 161177690.969\n",
            "198371/198371: [===============================>] - ETA 0.0s\n",
            "Training DAGMM... Epoch: 199, Loss: 160728840.577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj8GxRMlw2iA",
        "outputId": "f4f74418-31d4-4947-d46f-9fe2ef069abc"
      },
      "source": [
        "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "Precision : 0.9523, Recall : 0.9228, F-score : 0.9373\n",
            "ROC AUC score: 98.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Vv9k94m0w1pn",
        "outputId": "bf492922-d9ff-4b4e-dfae-a3322cc824a1"
      },
      "source": [
        "scores_in = scores[np.where(labels==0)[0]]\n",
        "scores_out = scores[np.where(labels==1)[0]]\n",
        "\n",
        "\n",
        "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
        "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
        "out_.plot.kde(ax=ax, legend=True)\n",
        "ax.grid(axis='x')\n",
        "ax.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}